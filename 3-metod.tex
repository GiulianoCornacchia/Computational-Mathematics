\section{Metodo Risolutivo}
\subsection{Primal-Dual Interior Point method}
L'intuizione principale dei metodi Primal-Dual è 
quella di considerare sia il problema di minimizzazione
 $P$ che il suo duale $D$ per ottenere un limite 
superiore $v(P)$ ed inferiore $v(D)$ della soluzione.
Da $P$ e $D$ si ricava quindi il sistema KKT \ref{eq:pkkt2} associato e una misura
 della distanza della soluzione attuale dall'ottimo, detta \textit{complementary gap};
  definita come la differenza fra il valore della funzione obiettivo in $P$ e in $D$, eventualmente normalizzata.
  
  Ad ogni iterazione, una volta trovata una soluzione per \ref{eq:pkkt2}, calcoliamo una nuova coppia di soluzioni primali/duali 
  e riduciamo il complementary gap.

  Con gli elementi presentati finora possiamo delineare la struttura dello pseudocodice del nostro metodo come segue:

\begin{algorithm}
\caption{pseudocodice Interior-Point Primal-Dual method}\label{alg:pseudo}
\begin{algorithmic}[1]
\Function{PDIP}{Q, q, A, b, eps, maxit}
\State \textit{inizializzare} $(x^0, \lambda_{eq}^0, \lambda_s^0) > 0$ 
\State $\mu^0 \gets (x^{0\intercal} \lambda_s^0)/n$
\State $\mathit{complementary\_gap}^0 \gets v(P(x^0)) - v(D(x^0,\lambda_{eq}^0,\lambda_s^0))$
\State $\sigma \in [0,1]$
\State $k \gets 0$
\BState \textbf{while} $k<maxit \;\; \land \;\; complementary\_gap^k \geq eps$

\State $\mu^{k+1} \gets \sigma \mu^{k}  $

\State \textit{risolvere} \;$
\begin{bmatrix}
2Q +X^{-1}S & A^\intercal\\
A & 0 \\
\end{bmatrix}\begin{bmatrix}\Delta x^{k+1} \\ \Delta \lambda_{eq}^{k+1}\end{bmatrix}= -
\begin{bmatrix}
    r_d-X^{-1}\mu^{k+1} e + \lambda_s\\r_p
\end{bmatrix}
$

\State $\Delta \lambda_s^{k+1} \gets X^{-1}(\mu^{k+1} e - S\Delta x^{k+1}) - \lambda_s^{k}$ 

\State \textit{calcolare} \; $(\alpha_x^{k+1}, \alpha_{\lambda_{eq}}^{k+1}, \alpha_{\lambda_{s}}^{k+1})$ 
\textit{tale che} $(x^{k+1},\lambda_{s}^{k+1})>0$


\State $(x^{k+1}, \lambda_{eq}^{k+1}, \lambda_{s}^{k+1}) \gets
(x^{k}, \lambda_{eq}^{k}, \lambda_{s}^{k}) + 
(\alpha_x^{k+1}\Delta_x^{k+1}, \alpha_{\lambda_{eq}}^{k+1}\Delta_{\lambda_{eq}}^{k+1}, \alpha_{\lambda_{s}}^{k+1}\Delta_{\lambda_{s}}^{k+1})$
\State $k \gets k + 1$
\State $\mathit{complementary\_gap}^k \gets v(P(x^k)) - v(D(x^k,\lambda_{eq}^k,\lambda_s^k))$
\EndFunction
\end{algorithmic}
\end{algorithm}

Nell'algoritmo sopra descritto sono cruciali la scelta della tripla iniziale (riga 2) e la scelta di $\alpha$ (riga 10); nelle sezioni seguenti descriveremo le nostre scelte progettuali a riguardo.
Le condizioni di stop scelte (riga 6) sono descritte in \ref{cap:stop}.

Mentre per la risoluzione della riga 8 utilizzeremo sia un approccio iterativo (GMRES) che diretto (versione modificata di LDL).
Entrambi i metodi verranno presentati nella sezione \ref{cap:RISLS}.

\subsection{Punto inziale}\label{cap:sp}
Il nostro obiettivo è trovare una tripla iniziale $(x^0, \lambda_{eq}^0, \lambda_s^0)$ che sia ammissibile per $P$ e $D$.
Di seguito presentiamo le scelte implementative per inizializzare i vettori della tripla.

\subsubsection*{Scelta $x^0$}
Per la scelta di $x^0$ sfruttiamo la struttura della matrice dei vincoli $A$; ogni $x_i$ appare in un solo vincolo ed il vincolo è definito come in \ref{eq:vincoli}, quindi: 

\begin{equation}\label{eq:startx}
  x_i=\frac{1}{|I^h|}  
\end{equation}
dove $I^h$ è l'insieme di indici, ovvero il simplesso, che contiene l'indice $i$.

\subsubsection*{Scelta $\lambda^0_{eq}$ e $\lambda^0_{s}$}
Inizializziamo $\lambda^0_{eq}$ con numeri positivi $\in (0,1)$; mentre per garantire l'ammissibilità duale troviamo $\lambda^0_s$ risolvendo 
$\nabla _x L(x^0, \lambda_{eq}^0, \lambda_s^0) = 0$.

\begin{equation}\label{eq:startl}
\overline{\lambda_s} = 2Qx^0+q+A^\intercal\lambda_{eq}^0
\end{equation}

Nel vettore $\lambda_s^0$ potrebbero essere presenti componenti negative, quindi definiamo $\Delta_s$ come:

\begin{equation}\label{eq:startdl}
\Delta_s = -(3/2)\;max(0, \underset{i}{min\;}\overline{\lambda_{s_i}})
\end{equation}

a questo punto scriviamo $\lambda_s^0 = \overline{\lambda_s} + e \Delta_s$. 


\subsection{Scelta dello step-size $\alpha$}
Scegliamo di utilizzare uno step-size distinto per ogni vettore della tripla $(x, \lambda_{eq}, \lambda_s)$.
Poichè vogliamo assicurare che, muovendoci lungo le direzioni trovate risolvendo il sistema \ref{eq:pkkt2}, i soli vincoli di non negatività in \ref{eq:nonneg} siano soddisfatti, scegliamo il massimo $\alpha$ tale che i vincoli non vengano violati.
Per il generico vettore $d \in \{x,\lambda_{eq},\lambda_s\} $ 
\begin{equation}\label{eq:alpha}
    \alpha^d_{max} = \min (1, \underset{i:\Delta d_i < 0}{min}-\frac{d_i}{\Delta d_i})
\end{equation}

E quindi scegliamo lo step $\alpha$ come segue:
\begin{equation}\label{eq:alphaFinale}
\alpha^d = \min(1, \eta \alpha^d_{max}) \;\;\;\;\;\; dove\;\;\; \eta \in [0.9, 1)
\end{equation}


\subsection{Criteri di stop}\label{cap:stop}
L'algoritmo può terminare sia perchè è stato raggiunto il numero massimo di iterazioni, oppure perchè il \textit{complementary gap} è minore di $\epsilon$, ovvero $v(P)-v(D)<\epsilon$.
Nella nostra analisi queste due soglie sono fissate rispetivvamente a $100$ e $10^{-14}$.


%\begin{subequations}
%\begin{equation}
%\|\nabla _x L(x, \lambda_{eq}, %\lambda_s)\|_2 \leq \epsilon_{d}
%\end{equation}
%\begin{equation}
%   \| Ax - b\|_2 \leq \epsilon_{p}
%\end{equation}
%\begin{equation}
%    \|XSe\|_2 \leq \epsilon_{s}
%\end{equation}
%\end{subequations}


\subsection{Risoluzione del sistema lineare}\label{cap:RISLS}
Per la risoluzione del sistema in \ref{eq:pkkt2} abbiamo deciso di utilizzare e confrontare due diversi approcci; uno iterativo (GMRES) e l'altro diretto (LDL).

GMRES è un metodo iterativo per la soluzioni di sistemi lineari $Ax=b$ non simmetrici di grandi dimensioni.
La soluzione esatta del sistema è $x_\ast = A^{-1}b$.
L'idea del metodo GMRES è di approssimare la soluzione $x_\ast$ al passo $n$ con un vettore $x_n \in K_n$, dove $K_n$ è il Krylov subspace $\langle b, Ab,\dots,A^{n-1}b\rangle$, che minimizza la norma del residuo $r_n = b - Ax_n$.
Questo si traduce in determinare $x_n$ risolvendo un least squares problem \cite{trefethen1997numerical}.

Mentre per la risoluzione diretta del sistema, usiamo una versione modificata di LDL per matrici simmetriche, implementata da \texttt{MATLAB} dall'operatore \textit{mldivide}.


\subsection{Convergenza del metodo}\label{cap:Convergenza}

I risultati teorici sulla convergenza dei metodi PDIP \cite{Nocedal2006Numerical} dimostrano che effettuando scelte appropriate sulla tripla iniziale $(x^0, \lambda_{eq}^0, \lambda_s^0)$, per $\epsilon > 0$ fissato, il limite superiore al numero delle iterazioni per garantire la convergenza dell'algoritmo è $log(n\; log(1/\epsilon))$ dove $n$ è l'input size del problema.

Il nostro metodo è di tipo \textit{infeasible}, 
ovvero ammette un punto iniziale che non soddisfi i vincoli in $P$ e $D$, ma data la scelta di uno step size diverso per il problema primale e duale, e l'aggiunta del centering parameter - scelte che in pratica aumentano la velocità di convergenza - ci aspettiamo che il nostro metodo converga comunque in un numero di iterazioni di $\approx 100$, per $n \rightarrow \infty$, come empiricamente osservato sui metodi di questa classe.

